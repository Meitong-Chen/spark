{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Q1:**"
      ],
      "metadata": {
        "id": "4s6MwfU2XhJ6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQHjkHYCsUxA",
        "outputId": "98e735be-cdf8-4451-def9-9fc3e026d4ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=5baccb352fcbb159c9b9d6e42eeff13f40d46cde493f256862de5e2ddb2a9d0c\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null #Install java\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz ## Install Apache Spark\n",
        "!tar xf spark-3.4.1-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I reinstall the pyspark with a exact version 3.1.2, cuz i had the problem with the link between PySpark and sql without doing this\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "!pip install pyspark==3.1.2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9IDicndNwqx",
        "outputId": "f4d22cc7-a9a6-47eb-ce0e-528da3692efa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark==3.1.2 in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.1.2) (0.10.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "# Define Java and Spark home path in Google Colab\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\""
      ],
      "metadata": {
        "id": "0B8juagltbTD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KtfLZdT3uN0-"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# from pyspark import SparkConf, SparkContext\n",
        "from datetime import datetime, date, timedelta\n",
        "from dateutil import relativedelta\n",
        "from pyspark.sql import SQLContext, Row\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.functions import to_timestamp, to_date\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import collect_list, collect_set, concat, first, array_distinct, col, size, expr\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"Flight Data Analysis\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "a8J2SIysjXQ6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flightData2015 = spark\\\n",
        "    .read\\\n",
        "    .option(\"inferSchema\", \"true\")\\\n",
        "    .option(\"header\", \"true\")\\\n",
        "    .csv(\"2015-summary.csv\")"
      ],
      "metadata": {
        "id": "fVIH_5xBG1ja"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flightData2015.take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8uGVvMwE1mW",
        "outputId": "6ec7545a-0ac2-4901-d8b9-3e57bb55c3f6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
              " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
              " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flightData2015.sort(\"count\").explain()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpkUbVC8Hs4Q",
        "outputId": "be1bdd4b-ad7d-4947-c496-6216a3ab8ad0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Sort [count#19 ASC NULLS FIRST], true, 0\n",
            "   +- Exchange rangepartitioning(count#19 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=33]\n",
            "      +- FileScan csv [DEST_COUNTRY_NAME#17,ORIGIN_COUNTRY_NAME#18,count#19] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
        "flightData2015.sort(\"count\").take(2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxgjoSueHvIB",
        "outputId": "046b3b9b-e219-430a-a699-65624bd5a572"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
              " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flightData2015.createOrReplaceTempView(\"flight_data_2015\")"
      ],
      "metadata": {
        "id": "0G7AYSILHxrT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sqlWay = spark.sql(\"\"\"\n",
        "SELECT DEST_COUNTRY_NAME, count(1)\n",
        "FROM flight_data_2015\n",
        "GROUP BY DEST_COUNTRY_NAME\n",
        "\"\"\")\n",
        "dataFrameWay = flightData2015\\\n",
        ".groupBy(\"DEST_COUNTRY_NAME\")\\\n",
        ".count()\n",
        "sqlWay.explain()\n",
        "dataFrameWay.explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9tP9aQ6H0z5",
        "outputId": "5d733be8-e3ff-4df9-f65b-df2c390b4c60"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[DEST_COUNTRY_NAME#17], functions=[count(1)])\n",
            "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#17, 5), ENSURE_REQUIREMENTS, [plan_id=55]\n",
            "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#17], functions=[partial_count(1)])\n",
            "         +- FileScan csv [DEST_COUNTRY_NAME#17] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
            "\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[DEST_COUNTRY_NAME#17], functions=[count(1)])\n",
            "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#17, 5), ENSURE_REQUIREMENTS, [plan_id=68]\n",
            "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#17], functions=[partial_count(1)])\n",
            "         +- FileScan csv [DEST_COUNTRY_NAME#17] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT max(count) from flight_data_2015\").take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LY9WRXSOSOB",
        "outputId": "848b33c0-9fa3-47e5-9733-e9ce06aa2530"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(max(count)=370002)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import max\n",
        "flightData2015.select(max(\"count\")).take(1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3n79jeiOSv8",
        "outputId": "8f5f29c3-b1ba-4609-dc97-4ae27ad20b52"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(max(count)=370002)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxSql = spark.sql(\"\"\"\n",
        "SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
        "FROM flight_data_2015\n",
        "GROUP BY DEST_COUNTRY_NAME\n",
        "ORDER BY sum(count) DESC\n",
        "LIMIT 5\n",
        "\"\"\")\n",
        "maxSql.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uhdtL01OV57",
        "outputId": "0f858694-4185-4956-8187-66464bbf4591"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----------------+\n",
            "|DEST_COUNTRY_NAME|destination_total|\n",
            "+-----------------+-----------------+\n",
            "|    United States|           411352|\n",
            "|           Canada|             8399|\n",
            "|           Mexico|             7140|\n",
            "|   United Kingdom|             2025|\n",
            "|            Japan|             1548|\n",
            "+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import desc\n",
        "flightData2015\\\n",
        ".groupBy(\"DEST_COUNTRY_NAME\")\\\n",
        ".sum(\"count\")\\\n",
        ".withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
        ".sort(desc(\"destination_total\"))\\\n",
        ".limit(5)\\\n",
        ".show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-brZxmLXOYhR",
        "outputId": "0b7afda6-50aa-48c0-8e5c-c093a1874bdb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----------------+\n",
            "|DEST_COUNTRY_NAME|destination_total|\n",
            "+-----------------+-----------------+\n",
            "|    United States|           411352|\n",
            "|           Canada|             8399|\n",
            "|           Mexico|             7140|\n",
            "|   United Kingdom|             2025|\n",
            "|            Japan|             1548|\n",
            "+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flightData2015\\\n",
        ".groupBy(\"DEST_COUNTRY_NAME\")\\\n",
        ".sum(\"count\")\\\n",
        ".withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
        ".sort(desc(\"destination_total\"))\\\n",
        ".limit(5)\\\n",
        ".explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4lh7e_EOcrM",
        "outputId": "3ea4ccc3-70be-426f-ef44-6713b243ee6a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- TakeOrderedAndProject(limit=5, orderBy=[destination_total#113L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#17,destination_total#113L])\n",
            "   +- HashAggregate(keys=[DEST_COUNTRY_NAME#17], functions=[sum(count#19)])\n",
            "      +- Exchange hashpartitioning(DEST_COUNTRY_NAME#17, 5), ENSURE_REQUIREMENTS, [plan_id=238]\n",
            "         +- HashAggregate(keys=[DEST_COUNTRY_NAME#17], functions=[partial_sum(count#19)])\n",
            "            +- FileScan csv [DEST_COUNTRY_NAME#17,count#19] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q2:**"
      ],
      "metadata": {
        "id": "uDsMIhIzX2p6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **(1) - a:**"
      ],
      "metadata": {
        "id": "LJ84HCquX5i7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import max, col\n",
        "\n",
        "# new Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Highest Average Temperature Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "temperature_df = spark.read.csv(\"GlobalLandTemperatures_GlobalLandTemperaturesByCountry.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# convert dt column to date type\n",
        "temperature_df = temperature_df.withColumn(\"year\", col(\"dt\").substr(0, 4))\n",
        "\n",
        "# deal with NA\n",
        "temperature_df = temperature_df.na.drop(subset=[\"AverageTemperature\"])\n",
        "\n",
        "\n",
        "max_temp = temperature_df.select(max(\"AverageTemperature\")).collect()[0][0]\n",
        "max_temp_record = temperature_df.filter(temperature_df[\"AverageTemperature\"] == max_temp)\n",
        "max_temp_record.select(\"Country\", \"year\", \"AverageTemperature\").show()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4CpFfgRX-YO",
        "outputId": "9bfe6e2d-bdab-4890-8ad7-d2d6b9ea8327"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+------------------+\n",
            "|Country|year|AverageTemperature|\n",
            "+-------+----+------------------+\n",
            "| Kuwait|2012| 38.84200000000001|\n",
            "+-------+----+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **(1) - b:**"
      ],
      "metadata": {
        "id": "4adv2suFYxIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import max, min, col, abs\n",
        "\n",
        "# new Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Temperature Change Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "temperature_df = spark.read.csv(\"GlobalLandTemperatures_GlobalLandTemperaturesByCountry.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# convert dt to date type and extract the year\n",
        "temperature_df = temperature_df.withColumn(\"year\", col(\"dt\").substr(0, 4))\n",
        "\n",
        "# drop na\n",
        "temperature_df = temperature_df.na.drop(subset=[\"AverageTemperature\"])\n",
        "\n",
        "# max &min\n",
        "temp_range_df = temperature_df.groupBy(\"Country\").agg(\n",
        "    max(\"AverageTemperature\").alias(\"MaxTemperature\"),\n",
        "    min(\"AverageTemperature\").alias(\"MinTemperature\")\n",
        ")\n",
        "\n",
        "# diff\n",
        "temp_range_df = temp_range_df.withColumn(\"TemperatureChange\", abs(col(\"MaxTemperature\") - col(\"MinTemperature\")))\n",
        "top_countries = temp_range_df.orderBy(col(\"TemperatureChange\").desc()).limit(10)\n",
        "top_countries.select(\"Country\", \"TemperatureChange\").show()\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-PAFPDlYy-4",
        "outputId": "1368935a-e8e7-4b9a-c040-a7dfd704ce2d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------------+\n",
            "|    Country| TemperatureChange|\n",
            "+-----------+------------------+\n",
            "| Kazakhstan|            49.163|\n",
            "|   Mongolia|48.157999999999994|\n",
            "|     Canada|            43.532|\n",
            "|    Finland|            40.332|\n",
            "|    Belarus|            39.338|\n",
            "|    Estonia|38.882999999999996|\n",
            "| Kyrgyzstan| 38.43599999999999|\n",
            "|North Korea|            38.342|\n",
            "|     Latvia|            38.063|\n",
            "|    Moldova|            38.012|\n",
            "+-----------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **(2) - a, b:**"
      ],
      "metadata": {
        "id": "dcSB2H9cY6fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall pyspark py4j\n",
        "!pip install pyspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 982
        },
        "id": "PhpRV5Kf8wmQ",
        "outputId": "e7dfe77d-c8eb-4566-99dd-51c0ce39f390"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: pyspark 3.1.2\n",
            "Uninstalling pyspark-3.1.2:\n",
            "  Would remove:\n",
            "    /usr/local/bin/beeline\n",
            "    /usr/local/bin/beeline.cmd\n",
            "    /usr/local/bin/docker-image-tool.sh\n",
            "    /usr/local/bin/find-spark-home\n",
            "    /usr/local/bin/find-spark-home.cmd\n",
            "    /usr/local/bin/find_spark_home.py\n",
            "    /usr/local/bin/load-spark-env.cmd\n",
            "    /usr/local/bin/load-spark-env.sh\n",
            "    /usr/local/bin/pyspark\n",
            "    /usr/local/bin/pyspark.cmd\n",
            "    /usr/local/bin/pyspark2.cmd\n",
            "    /usr/local/bin/run-example\n",
            "    /usr/local/bin/run-example.cmd\n",
            "    /usr/local/bin/spark-class\n",
            "    /usr/local/bin/spark-class.cmd\n",
            "    /usr/local/bin/spark-class2.cmd\n",
            "    /usr/local/bin/spark-shell\n",
            "    /usr/local/bin/spark-shell.cmd\n",
            "    /usr/local/bin/spark-shell2.cmd\n",
            "    /usr/local/bin/spark-sql\n",
            "    /usr/local/bin/spark-sql.cmd\n",
            "    /usr/local/bin/spark-sql2.cmd\n",
            "    /usr/local/bin/spark-submit\n",
            "    /usr/local/bin/spark-submit.cmd\n",
            "    /usr/local/bin/spark-submit2.cmd\n",
            "    /usr/local/bin/sparkR\n",
            "    /usr/local/bin/sparkR.cmd\n",
            "    /usr/local/bin/sparkR2.cmd\n",
            "    /usr/local/lib/python3.10/dist-packages/pyspark-3.1.2.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/pyspark/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled pyspark-3.1.2\n",
            "Found existing installation: py4j 0.10.9\n",
            "Uninstalling py4j-0.10.9:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/py4j-0.10.9.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/py4j/*\n",
            "    /usr/local/share/py4j/py4j0.10.9.jar\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled py4j-0.10.9\n",
            "Collecting pyspark\n",
            "  Using cached pyspark-3.5.1-py2.py3-none-any.whl\n",
            "Collecting py4j==0.10.9.7 (from pyspark)\n",
            "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.7 pyspark-3.5.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "py4j",
                  "pyspark"
                ]
              },
              "id": "ba7291ab59684cf0b2e3b10bce4e9ac3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, year, lit, max, min\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"HW2_part2\").getOrCreate()\n",
        "co2_data_path = 'CO2 emissions per capita per country.csv'\n",
        "temp_data_path = 'GlobalLandTemperatures_GlobalLandTemperaturesByCountry.csv'\n",
        "\n",
        "# read and process global temperature data\n",
        "globalLandTemp = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(temp_data_path)\n",
        "globalLandTemp = globalLandTemp.withColumn(\"Year\", year(globalLandTemp.dt))\n",
        "globalLandTemp.createOrReplaceTempView(\"temp_data\")\n",
        "\n",
        "# CO2 emission data\n",
        "co2 = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(co2_data_path)\n",
        "\n",
        "# reshape CO2 data into a long format\n",
        "schema = StructType([\n",
        "    StructField(\"Country\", StringType(), True),\n",
        "    StructField(\"Year\", StringType(), True),\n",
        "    StructField(\"CO2_per_capita\", DoubleType(), True)\n",
        "])\n",
        "co2_long = spark.createDataFrame([], schema)\n",
        "\n",
        "year_columns = co2.columns[2:]\n",
        "for year in year_columns:\n",
        "    df_year = co2.select(\n",
        "        col(\"Country Name\").alias(\"Country\"),\n",
        "        lit(year).alias(\"Year\"),\n",
        "        col(year).cast(DoubleType()).alias(\"CO2_per_capita\")\n",
        "    )\n",
        "    co2_long = co2_long.unionByName(df_year)\n",
        "\n",
        "# filter and aggregate temperature data to get yearly changes per country\n",
        "filtered_globalLandTemp = globalLandTemp.filter((col(\"Year\") >= 1960) & (col(\"Year\") <= 2014))\n",
        "df_temp_change = filtered_globalLandTemp.groupBy('Country', 'Year').agg(\n",
        "    (max('AverageTemperature') - min('AverageTemperature')).alias('TemperatureChange')\n",
        ").orderBy(\"Year\", \"Country\")\n",
        "\n",
        "# cast year to string to match with CO2 data format\n",
        "df_temp_change = df_temp_change.withColumn(\"Year\", df_temp_change[\"Year\"].cast(StringType()))\n",
        "co2_long = co2_long.withColumn(\"Year\", co2_long[\"Year\"].cast(StringType()))\n",
        "\n",
        "merged_df = df_temp_change.join(co2_long, [\"Country\", \"Year\"], \"inner\")\n",
        "sorted_merged_df = merged_df.orderBy(\"Year\", \"Country\")\n",
        "non_null = sorted_merged_df.filter(sorted_merged_df.CO2_per_capita.isNotNull())\n",
        "\n",
        "# make sure all data is numeric before correlation calculation\n",
        "non_null = non_null.withColumn('CO2_per_capita', non_null['CO2_per_capita'].cast(DoubleType()))\n",
        "non_null = non_null.withColumn('TemperatureChange', non_null['TemperatureChange'].cast(DoubleType()))\n",
        "\n",
        "print(\"Sorted and Cleaned Merged DataFrame:\")\n",
        "non_null.show()\n",
        "\n",
        "# corr\n",
        "correlation = non_null.stat.corr('CO2_per_capita', 'TemperatureChange')\n",
        "print(\"Correlation coefficient:\", correlation)\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9PvSpv2vifQ",
        "outputId": "b6a4ab67-5127-4171-8a90-17a39dd68ec8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorted and Cleaned Merged DataFrame:\n",
            "+--------------+----+------------------+--------------+\n",
            "|       Country|Year| TemperatureChange|CO2_per_capita|\n",
            "+--------------+----+------------------+--------------+\n",
            "|   Afghanistan|1960|            24.789|   0.046059897|\n",
            "|       Albania|1960|            18.404|   1.258194928|\n",
            "|       Algeria|1960|22.242000000000004|   0.553763777|\n",
            "|        Angola|1960| 5.777000000000001|   0.097471604|\n",
            "|     Argentina|1960|            14.945|   2.367473032|\n",
            "|     Australia|1960|            14.335|   8.582936643|\n",
            "|       Austria|1960|            19.263|   4.373318828|\n",
            "|       Bahrain|1960|            16.924|   3.544478443|\n",
            "|      Barbados|1960|2.0559999999999974|   0.746296641|\n",
            "|       Belgium|1960|            14.815|   9.941594074|\n",
            "|        Belize|1960| 4.963999999999999|   0.477971846|\n",
            "|         Benin|1960|             4.073|   0.066354063|\n",
            "|       Bolivia|1960|             5.279|    0.27203787|\n",
            "|        Brazil|1960|             2.314|   0.649630979|\n",
            "|      Bulgaria|1960|            19.895|   2.833901121|\n",
            "|  Burkina Faso|1960| 6.314000000000004|   0.009111902|\n",
            "|      Cambodia|1960| 5.140000000000001|   0.041012378|\n",
            "|      Cameroon|1960|2.5859999999999985|   0.052423483|\n",
            "|        Canada|1960|            35.279|   10.77084729|\n",
            "|Cayman Islands|1960|4.3919999999999995|   1.398728544|\n",
            "+--------------+----+------------------+--------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Correlation coefficient: 0.3690790634498884\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}